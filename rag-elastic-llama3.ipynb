{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Install required dependencies for LlamaIndex and Elasticsearch",
   "id": "89777ad31dbed1fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-cli\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-embeddings-elasticsearch\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-embeddings-ollama\n",
    "!pip install llama-index-indices-managed-llama-cloud\n",
    "!pip install llama-index-legacy\n",
    "!pip install llama-index-llms-ollama\n",
    "!pip install llama-index-readers-elasticsearch\n",
    "!pip install llama-index-readers-file\n",
    "!pip install llama-index-readers-llama-parse\n",
    "!pip install llama-index-vector-stores-elasticsearch\n",
    "!pip install llama-parse\n",
    "!pip install llamaindex-py-client\n"
   ],
   "id": "c33613aab394f7d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Import packages",
   "id": "ac6d83485e810a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.vector_stores.elasticsearch import ElasticsearchStore\n",
    "from llama_index.core import VectorStoreIndex, QueryBundle\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Document, Settings\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "import json"
   ],
   "id": "34918747fcd940e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Prompt user to provide Cloud ID and API Key",
   "id": "8fa1c4a9752298f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n",
    "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")"
   ],
   "id": "5093cfb9b0688aa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Prepare documents for chunking and ingestion",
   "id": "24b1abd62ac05d07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/datasets/workplace-documents.json\"\n",
    "\n",
    "response = urlopen(url)\n",
    "workplace_docs = json.loads(response.read())\n",
    "\n",
    "\n",
    "# Building Document required by LlamaIndex.\n",
    "documents = [Document(text=doc['content'],\n",
    "                          metadata={\"name\": doc['name'],\"summary\": doc['summary'],\"rolePermissions\": doc['rolePermissions']})\n",
    "                 for doc in workplace_docs]"
   ],
   "id": "a7d3aa4eb6faaa2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define Elasticsearch and ingest pipeline in LlamaIndex for document processing. Use Llama3 for generating embeddings.",
   "id": "d39dd6e65a8ea3df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "es_vector_store = ElasticsearchStore(index_name=\"workplace_index\",\n",
    "                                     vector_field='content_vector',\n",
    "                                     text_field='content',\n",
    "                                     es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "                                     es_api_key=ELASTIC_API_KEY)\n",
    "# Embedding Model to do local embedding using Ollama.\n",
    "ollama_embedding = OllamaEmbedding(\"llama3\")\n",
    "# LlamaIndex Pipeline configured to take care of chunking, embedding\n",
    "# and storing the embeddings in the vector store.\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=100),\n",
    "        ollama_embedding\n",
    "    ], vector_store=es_vector_store\n",
    ")"
   ],
   "id": "5abfbf4f8a1b5a35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Execute pipeline, which will chunk the data, generate embeddings using Llama3 and ingest into Elasticsearch index, with embeddings in a dense vector field.",
   "id": "4b343865fcd1a742"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipeline.run(show_progress=True,documents=documents)",
   "id": "3b498e38cbc25453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Define LLM settings. This connects to your local LLM. Please refer to https://ollama.com/library/llama3 for details on steps to run Llama3 locally. \n",
    "#### If you have sufficient resources (atleast >64 GB Ram and GPU available) then you could try the 70B parameter version of Llama3. "
   ],
   "id": "ee297886dd4e6396"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Settings.embed_model = ollama_embedding\n",
    "local_llm = Ollama(model=\"llama3\")"
   ],
   "id": "eaee053103a55f39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Semantic search and integrate with Llama3. ",
   "id": "e22ed3f0b11c4bb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "index = VectorStoreIndex.from_vector_store(es_vector_store)\n",
    "query_engine = index.as_query_engine(local_llm, similarity_top_k=10)\n",
    "\n",
    "# Customer Query\n",
    "query = \"What are the organizations sales goals?\"\n",
    "bundle = QueryBundle(query_str=query,\n",
    "embedding=Settings.embed_model.get_query_embedding(query=query))\n",
    "\n",
    "response = query_engine.query(bundle)\n",
    "\n",
    "print(response.response)"
   ],
   "id": "22e96628ac44f20d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
